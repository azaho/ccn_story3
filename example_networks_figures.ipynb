{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041aa425-dadf-4d6c-a945-994c76b54804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, time, random\n",
    "import hashlib, torch, math, pathlib\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.optimize import curve_fit\n",
    "import importlib, importlib.util, os\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "targetinput_color = \"#006838\"\n",
    "distractor_color = \"#97211F\"\n",
    "analyze_network = \"final\"  # options: \"best\", \"final\", <parameter update step no>\n",
    "noise_amplitude = 0.1  # if run analyses with noise, noise amplitude\n",
    "distractor_probability = 0.0\n",
    "show_figures = True  # True if running in jupyter notebook; False if running a .py file\n",
    "running_from_data = True # True if code is running in the data folder next to the model.pth. if false, must run from training file\n",
    "\n",
    "directories = [\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r1/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r3/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r5/\",\n",
    "    \"data/hdgating_and_inversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdgating_and_reshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\",\n",
    "    \"data/hdgatingCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdinversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\",\n",
    "    \"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r1/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp0.0_r0/\",\n",
    "]\n",
    "directories = [\n",
    "    f\"data/hdinversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdgating_and_inversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_rGP60_{i}/\" for i in range(3)\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdgating_and_inversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdgating_and_inversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_rGP40_{i}/\" for i in range(3)\n",
    "]\n",
    "\n",
    "directories = [\n",
    "    f\"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in [1, 9, 7, 6, 5]\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdgatingCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdgating_and_reshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r{i}/\" for i in [0, 2, 3]\n",
    "]\n",
    "directories += [\n",
    "    f\"data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "\n",
    "directories = [\n",
    "    f\"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in [5]\n",
    "]\n",
    "\n",
    "#directories = [\n",
    "#    f\"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(10)\n",
    "#]\n",
    "\n",
    "directories = [\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r1/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r3/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r5/\",\n",
    "    \"data/hdgating_and_inversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdgating_and_reshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\",\n",
    "    \"data/hdgatingCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdinversionCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\",\n",
    "    \"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\",\n",
    "    \"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r1/\",\n",
    "    \"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp0.0_r0/\",\n",
    "]\n",
    "\n",
    "directories = [\n",
    "    f\"data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(30)\n",
    "]\n",
    "\n",
    "figure_dir_prefix = \"paper_figures/example_networks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a973c4-286c-447b-9864-96870ab72075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_pref_ave(units_i, timestep_from, timestep_to, to=1, data=None, round_prefs=False, smoothing_constant=0.01):\n",
    "    \"\"\"\n",
    "        Calculate the preferred direction of a given neuron across a window given\n",
    "    \"\"\"\n",
    "    if data is None: data = ao_data\n",
    "    w = torch.sum(data[timestep_from:timestep_to][:, units_i], dim=(0, 4-to)).detach().numpy()\n",
    "    a = np.angle(np.sum(w*np.exp(1j*(np.arange(ORI_SET_SIZE)/ORI_SET_SIZE*2*np.pi)).reshape(1, -1), axis=1)/ (np.sum(np.abs(w), axis=1)+smoothing_constant)) * 180 / np.pi\n",
    "    a[a<0] = a[a<0]+360\n",
    "    a = torch.tensor(a)\n",
    "    if round_prefs: a = torch.round(a)\n",
    "    return a\n",
    "    \n",
    "def generate_behavioral_figure(behavior_repeats = 10):\n",
    "    \"\"\"\n",
    "        Generates the error rates of the network across the different \"time to distractor\"\n",
    "        values and the Near and Far conditions\n",
    "\n",
    "        behavior_repeats is the number of samples for every condition\n",
    "    \"\"\"\n",
    "    delay1s = np.array([10, 15, 20, 25, 30, 90])\n",
    "    noise_amplitude_c = noise_amplitude*1\n",
    "    \n",
    "    def make_all_integer_directions_batch(delay0, delay1, delay2, resolution=1, distractor_probability=1, repeats=1, condition='near'):\n",
    "        batch = []  # inputs in the batch\n",
    "        batch_labels = []  # target outputs in the batch\n",
    "        output_masks = []  # masks in the batch\n",
    "        for i in range(repeats):\n",
    "            for direction1 in np.arange(8)/8*360:\n",
    "                for direction2 in np.arange(8)/8*360:\n",
    "                    diff = direction2-direction1\n",
    "                    if diff < -180: diff += 360\n",
    "                    if diff > 180: diff -= 360\n",
    "                    diff = abs(diff)\n",
    "                    if condition == 'near' and diff != 45: continue\n",
    "                    if condition == 'far' and diff != 135 and diff != 180: continue\n",
    "                    \n",
    "                    i_full, o_full, b_mask = task._make_trial(direction1, direction2, delay0, delay1, delay2, distractor_probability=distractor_probability)\n",
    "                    batch.append(i_full.unsqueeze(0))\n",
    "                    batch_labels.append(o_full.unsqueeze(0))\n",
    "                    output_masks.append(b_mask.unsqueeze(0))\n",
    "        return torch.cat(batch), torch.cat(batch_labels), torch.cat(output_masks)\n",
    "        \n",
    "    repeats = behavior_repeats\n",
    "    errors = {'near': {'rates': [], 'angles': []}, 'far': {'rates': [], 'angles': []}}\n",
    "    for condition in ['near', 'far']:\n",
    "        for delay1 in delay1s:\n",
    "            ao_input, ao_target, ao_mask = make_all_integer_directions_batch(delay0, delay1, delay2, distractor_probability=1, repeats=repeats, condition=condition)\n",
    "            ao_noise_mask = task.get_noise_mask(delay0, delay1, delay2)\n",
    "            ao_noise_mask = ao_noise_mask.repeat(ao_input.shape[0], 1).unsqueeze(2).repeat(1, 1, model.dim_recurrent)  # convert to (batch_size, total_time, dim_recurrent)\n",
    "            ao_noise = torch.randn_like(ao_noise_mask) * ao_noise_mask * noise_amplitude_c\n",
    "            ao_output, ao_h = model.forward(ao_input, noise=ao_noise)\n",
    "            t5 = delay0 + delay2 + task_parameters[\"show_direction_for\"]\n",
    "            t6 = t5 + task_parameters[\"show_cue_for\"]\n",
    "            _, _, e, _ = task.calculate_errors(ao_target, ao_output, ao_mask, t5, t6)\n",
    "            errors[condition]['angles'].append(e)\n",
    "\n",
    "            o1_o, _ = task.convert_sincos_to_angles(ao_output, t5, t6)\n",
    "            o1_t, _ = task.convert_sincos_to_angles(ao_target, t5, t6)\n",
    "            wrong = torch.minimum(torch.minimum(torch.abs(o1_o-o1_t), torch.abs(o1_o-o1_t+360)), torch.abs(o1_o-o1_t-360))>22.5\n",
    "            wrong = torch.sum(wrong, dim=1)>wrong.shape[1]/2\n",
    "            wrong = torch.sum(wrong)/wrong.shape[0]*100\n",
    "            errors[condition]['rates'].append(wrong)\n",
    "\n",
    "    delay1s_labels = delay1s.copy()\n",
    "    delay1s_labels[-1]=40\n",
    "    min_indices = [0, 2, 4, 5]\n",
    "\n",
    "    for suffix2, condition in {'': 'rates', '_angles': 'angles'}.items():\n",
    "        for suffix1, indices in {'': range(len(delay1s)), '_min': min_indices}.items():\n",
    "            fig = plt.figure(figsize=(3, 3))\n",
    "            plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 16})\n",
    "            plt.plot(delay1s_labels[indices], np.array(errors['near'][condition])[indices], \"k-\", linewidth=2, marker=\"o\", markersize=9, label=\"Near\")\n",
    "            plt.plot(delay1s_labels[indices], np.array(errors['far'][condition])[indices], \"--\", color=\"k\", linewidth=2, marker=\"^\", markersize=10, label=\"Far\")\n",
    "            plt.xlabel(\"time to distractor (ms)\")\n",
    "            if condition == 'rates':\n",
    "                plt.ylabel(\"error rate (%)\")\n",
    "            else:\n",
    "                plt.ylabel(\"average error\")\n",
    "                plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%.1f째'))\n",
    "                \n",
    "            plt.legend()\n",
    "            plt.ylim(0, max(max(errors['near'][condition]), max(errors['far'][condition]))*1.1)\n",
    "            plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "            plt.xlim(6.5, 43.5)\n",
    "            plt.xticks(delay1s_labels[min_indices], delay1s[min_indices]*10)\n",
    "            #plt.ylim(-1, 24)\n",
    "            plt.gca().spines['top'].set_visible(False)\n",
    "            plt.gca().spines['right'].set_visible(False)\n",
    "            plt.savefig(f'{figure_dir_prefix}{directory[5:-1]}_behavior{suffix1}{suffix2}.pdf', bbox_inches='tight')\n",
    "            plt.clf()\n",
    "\n",
    "def generate_activity_figure(vmax=1, trial_input=16, trial_distractor=20):\n",
    "    \"\"\"\n",
    "        Generate a heatmap of activities of all artificial neurons\n",
    "        for an example trial\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "    \n",
    "    timesteps_to_take = np.concatenate((range((t3+t4)//2), range(t4, (t5+t6)//2)))\n",
    "    data_to_show = ao_data[0:t5, torch.cat((DT_i, R1_i, )), trial_input, trial_distractor].T\n",
    "\n",
    "    _targetinput_color = \"#777\"\n",
    "    _distractor_color = \"#111\"\n",
    "    \n",
    "    plt.imshow(data_to_show*1, cmap='gray_r', vmin=0, vmax=vmax,interpolation='nearest', aspect='auto')\n",
    "    ax = plt.gca()\n",
    "    ax.axvspan(t1, t2, facecolor=_targetinput_color, alpha=0.2)\n",
    "    ax.axvspan(t3, t4, facecolor=_distractor_color, alpha=0.2)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    handles.append(mpatches.Patch(color=_targetinput_color, alpha=0.3, label='target'))\n",
    "    handles.append(mpatches.Patch(color=_distractor_color, alpha=0.3, label='distractor'))\n",
    "    plt.ylabel(\"neuron #\")\n",
    "    plt.xlabel(\"time\")\n",
    "    cb = plt.colorbar(ticks=[0, vmax])\n",
    "    cb.set_label(\"activity\", labelpad=-10)\n",
    "    plt.xticks([0, t1, t3, t5])\n",
    "    plt.xticks([])\n",
    "    plt.xlim(10, t5-60)\n",
    "    ax.legend(fontsize=12, handles=handles, loc=1)\n",
    "    plt.yticks([0, 20, 40, 60, 80, 100])\n",
    "    plt.yticks([])\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    plt.savefig(f'{figure_dir_prefix}{directory[5:-1]}_activity.pdf', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "def generate_prefchange_figure():\n",
    "    \"\"\"\n",
    "        Generates the histogram of absolute directional preference change of the artificial neurons\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "    ylim = 99\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    bins = 5\n",
    "    units_i = R1_i\n",
    "    cue_from, cue_to = t1, t2\n",
    "    delay_from, delay_to = t2+20, t3\n",
    "    prefc = _calc_pref_ave(units_i, cue_from, cue_to, to=1)-_calc_pref_ave(units_i, delay_from, delay_to, to=1)\n",
    "    prefc[prefc<-180] = prefc[prefc<-180]+360\n",
    "    prefc[prefc>180] = prefc[prefc>180]-360\n",
    "    prefc = torch.abs(prefc)\n",
    "    hist = torch.histc(prefc, bins = bins, min = 0, max = 180)\n",
    "    hist /= torch.sum(hist)\n",
    "    x = torch.arange(bins)/bins * 180\n",
    "    ax.bar(x*1.033+16, hist*100, align='center', width=180/bins/1.05, color=\"#444\")\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d째'))\n",
    "    ax.set_xlabel('preference change')\n",
    "    ax.set_ylabel('% of neurons')\n",
    "    ax.set_xticks(torch.arange(0, 181, 45))\n",
    "    ax.set_ylim(0, ylim)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{figure_dir_prefix}{directory[5:-1]}_prefchange.pdf', bbox_inches='tight')\n",
    "    plt.clf()\n",
    "\n",
    "def generate_gif_animation():\n",
    "    \"\"\"\n",
    "        Generates the gif animation of the activities of artificial neurons\n",
    "        in the network for one example trial. A visualization of bump attractor & dynamis\n",
    "    \"\"\"\n",
    "    #TWO RINGS\n",
    "    def make_all_integer_directions_batch(delay0, delay1, delay2, resolution=100, distractor_probability=1):\n",
    "        batch = []  # inputs in the batch\n",
    "        batch_labels = []  # target outputs in the batch\n",
    "        output_masks = []  # masks in the batch\n",
    "        for direction1 in np.arange(resolution)/resolution*360:\n",
    "            for direction2 in [0]:\n",
    "                i_full, o_full, b_mask = task._make_trial(direction1, direction2, delay0, delay1, delay2, distractor_probability=distractor_probability)\n",
    "                batch.append(i_full.unsqueeze(0))\n",
    "                batch_labels.append(o_full.unsqueeze(0))\n",
    "                output_masks.append(b_mask.unsqueeze(0))\n",
    "        return torch.cat(batch), torch.cat(batch_labels), torch.cat(output_masks)\n",
    "    ao_input, ao_target, ao_mask = make_all_integer_directions_batch(delay0, delay1, delay2, distractor_probability=0)\n",
    "    ao_noise_mask = task.get_noise_mask(delay0, delay1, delay2)\n",
    "    ao_noise_mask = ao_noise_mask.repeat(ao_input.shape[0], 1).unsqueeze(2).repeat(1, 1, model.dim_recurrent)  # convert to (batch_size, total_time, dim_recurrent)\n",
    "    ao_noise = torch.randn_like(ao_noise_mask) * ao_noise_mask * 0.1\n",
    "    ao_output, ao_h = model.forward(ao_input, noise=ao_noise)\n",
    "    \n",
    "    def update(t, trial=50):\n",
    "        ax.clear()\n",
    "        r = 1.1\n",
    "        units_i = R1_i\n",
    "        n_bars = len(units_i)\n",
    "        theta = calc_pref(units_i, t3-1, to=1)\n",
    "        x = np.cos(theta*np.pi/180)*r\n",
    "        y = np.sin(theta*np.pi/180)*r\n",
    "        z = np.zeros_like(x)\n",
    "        dx = dy = 0.1 * np.ones_like(z)\n",
    "        dz = torch.maximum(ao_h[trial, t, :][units_i], torch.tensor(0)).detach().numpy()*0.5\n",
    "        if t>=t1 and t<=t2:\n",
    "            ax.bar3d([0], [0], [0], [0.25], [0.25], [0.1], color=\"red\")\n",
    "        ax.bar3d(x, y, z+1, dx, dy, dz, color=\"lightgreen\")\n",
    "        r = 1\n",
    "        units_i = torch.tensor([x for x in range(100) if x not in R1_i])\n",
    "        n_bars = len(units_i)\n",
    "        if n_bars>0:\n",
    "            theta = calc_pref(units_i, t2-1, to=1)\n",
    "            x = np.cos(theta*np.pi/180)*r\n",
    "            y = np.sin(theta*np.pi/180)*r\n",
    "            z = np.zeros_like(x)\n",
    "            dx = dy = 0.1 * np.ones_like(z)\n",
    "            dz = torch.maximum(ao_h[trial, t, :][units_i], torch.tensor(0)).detach().numpy()*0.5\n",
    "            ax.bar3d(x, y, z, dx, dy, dz, color=\"lightblue\")\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.axis('off')\n",
    "        ax.set_zlim(0, 1.5)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ani = FuncAnimation(fig, update, frames=range(t1-10, t3), repeat=True)\n",
    "    writer = PillowWriter(fps=7)\n",
    "    ani.save(f'{figure_dir_prefix}{directory[5:-1]}_animation.gif', writer=writer)\n",
    "    plt.clf()\n",
    "\n",
    "def generate_functional_connectivity_plot(dir_prefix=None, sd=False):\n",
    "    \"\"\"\n",
    "        Generates the plot\n",
    "        if sd is True, the shaded region represents SD of the distribution within each bin;\n",
    "        otherwise, it's 95%CI\n",
    "    \"\"\"\n",
    "    if dir_prefix is None:\n",
    "        dir_prefix=figure_dir_prefix\n",
    "    lim = 0.1\n",
    "    # function that will be fit to the averaged connection weights\n",
    "    def cosine_fit(x, a, b):\n",
    "        return a * np.cos(x * np.pi / 180) + b\n",
    "    \n",
    "    def get_connplot_graph(units1_id=None, unit1_pref=None, units2_id=None, unit2_pref=None, sm=0):\n",
    "        weight_matrix = None  # different models may store weights differently\n",
    "        try:\n",
    "            weight_matrix = model.fc_h2ah.weight\n",
    "        except:\n",
    "            weight_matrix = model.W_h_ah\n",
    "        distances_weights = {}\n",
    "        distances = []\n",
    "        weights = []\n",
    "        for i in range(len(units1_id)):\n",
    "            for j in range(len(units2_id)):\n",
    "                for k in range(-sm // 2, sm // 2 + 1):\n",
    "                    if j == i: continue\n",
    "                    diff = (unit2_pref[j] - unit1_pref[i]).item()\n",
    "                    if diff > 180: diff -= 360\n",
    "                    if diff < -180: diff += 360\n",
    "                    diff += k\n",
    "                    w_ij = weight_matrix[units2_id[j], units1_id[i]]\n",
    "                    distances.append(diff)\n",
    "                    weights.append(w_ij.item())\n",
    "        return np.array(distances), np.array(weights)\n",
    "    \n",
    "    def get_connplot_iu_graph(units_id, unit_pref, sm=0):\n",
    "        weight_matrix = None  # different models may store weights differently\n",
    "        try:\n",
    "            weight_matrix = model.fc_x2ah.weight\n",
    "        except:\n",
    "            weight_matrix = model.W_x_ah\n",
    "        distances_weights = {}\n",
    "        distances = []\n",
    "        weights = []\n",
    "        for i in range(len(units_id)):\n",
    "            for j in range(task_parameters[\"input_direction_units\"]):\n",
    "                for k in range(-sm // 2, sm // 2 + 1):\n",
    "                    # if j == i: continue\n",
    "                    diff = (unit_pref[i] - round(360 * j / task_parameters[\"input_direction_units\"])).item()\n",
    "                    if diff > 180: diff -= 360\n",
    "                    if diff < -180: diff += 360\n",
    "                    diff += k\n",
    "    \n",
    "                    w_ij = weight_matrix[units_id[i], j]\n",
    "                    distances.append(diff)\n",
    "                    weights.append(w_ij.item())\n",
    "        return np.array(distances), np.array(weights)\n",
    "    \n",
    "    def plot_weights(timestep_from, timestep_to, ax, weights='input', color=\"green\", lim=0.1, fit_curves=True, ax_i=-1):\n",
    "        if weights=='input':\n",
    "            x, y = get_connplot_iu_graph(R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1))\n",
    "        elif weights=='recurrent':\n",
    "            x, y = get_connplot_graph(R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1), R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1))\n",
    "        bins = np.linspace(-180, 180, 20)\n",
    "        x_binned = []\n",
    "        y_binned = []\n",
    "        sem_binned = []\n",
    "        bin_ids = np.digitize(x, bins)\n",
    "        for i in range(1, len(bins)):\n",
    "            x_binned.append(np.mean(x[bin_ids == i]))\n",
    "            y_binned.append(np.mean(y[bin_ids == i]))\n",
    "            sem_binned.append(scipy.stats.sem(y[bin_ids == i]) if not sd else np.std(y[bin_ids == i]))\n",
    "        x_binned, y_binned, sem_binned = np.array(x_binned), np.array(y_binned), np.array(sem_binned) * 1.96\n",
    "        ax.axhline(y=0.0, color='gray', linestyle='--', linewidth=2)\n",
    "        ax.fill_between(x_binned, y_binned - sem_binned, y_binned + sem_binned, color=color, alpha=0.3, linewidth=0)\n",
    "        ax.plot(x_binned, y_binned, \"-\", color=color, linewidth=3)\n",
    "        ax.set_xlim(-180, 180)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_ylim(-lim * 1.1, lim * 1.1)\n",
    "        ax.set_yticks([-lim, 0, lim])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([-180, 0, 180])\n",
    "        ax.xaxis.set_major_formatter(FormatStrFormatter('%d째'))\n",
    "        if ax_i>=1:\n",
    "            ax.set_xlabel(\"preferred angle difference\\nbetween two units\")\n",
    "        ax.set_yticks([-lim, 0, lim])\n",
    "\n",
    "        if not np.isfinite(x_binned).all():\n",
    "            print(\"x\")\n",
    "            print(x_binned)\n",
    "            print(weights)\n",
    "        if not np.isfinite(y_binned).all():\n",
    "            print(\"y\")\n",
    "            print(y_binned)\n",
    "        params, covariance = curve_fit(cosine_fit, x_binned, y_binned)\n",
    "        y_fit = cosine_fit(x_binned, *params)\n",
    "        if fit_curves:\n",
    "            ax.plot(x_binned, y_fit, \"--\", color=color, linewidth=3, label=f\"$a_{ax_i}$ = {params[0]:.3f}\")\n",
    "        return params\n",
    "\n",
    "    for fit_curves in [True, False]:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "        ax1 = fig.add_subplot(2, 2, 1)\n",
    "        ax2 = fig.add_subplot(2, 2, 2)\n",
    "        ax3 = fig.add_subplot(2, 2, 3)\n",
    "        ax4 = fig.add_subplot(2, 2, 4)\n",
    "        ax1.set_ylabel(\"weight between two\\nrecurrent units\")\n",
    "        ax3.set_ylabel(\"weight between input\\nand recurrent unit\")\n",
    "        params_recurrent_cue = plot_weights(t1, t2, ax1, weights='recurrent', color=\"gray\", lim=lim, fit_curves=fit_curves)\n",
    "        params_recurrent_delay = plot_weights(t2+20, t3, ax2, weights='recurrent', color=\"k\", lim=lim, fit_curves=fit_curves, ax_i=0)\n",
    "        params_input_cue = plot_weights(t1, t2, ax3, weights='input', color=targetinput_color, lim=lim, fit_curves=fit_curves, ax_i=1)\n",
    "        params_input_delay = plot_weights(t2+20, t3, ax4, weights='input', color=distractor_color, lim=lim, fit_curves=fit_curves, ax_i=2)\n",
    "        if fit_curves: \n",
    "            for ax in [ax2, ax3, ax4]: ax.legend(loc='lower center')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_prefix+directory[5:-1] + f\"_connectivity{'_nofit' if not fit_curves else ''}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.clf()\n",
    "    \n",
    "    factors = {\n",
    "        \"structural_factor\": params_recurrent_delay[0] / params_input_cue[0],\n",
    "        \"functional_factor\": params_input_cue[0] / params_input_delay[0]\n",
    "    }\n",
    "    with open(dir_prefix+directory[5:-1] + '_connectivity.json', 'w') as f:\n",
    "        json.dump(factors, f, indent=4)\n",
    "    # Also save to the data_json dir for use in radius.ipynb\n",
    "    with open(\"data_json/\"+directory[5:-1] + '_connectivity.json', 'w') as f:\n",
    "        json.dump(factors, f, indent=4)\n",
    "\n",
    "def generate_exampleneuronfr_figure(neuron_i = 19):\n",
    "    # Show one selected neuron\n",
    "    colors = plt.cm.viridis(np.linspace(0, .9, 4))\n",
    "    \n",
    "    fig = plt.figure(figsize=(4, 3))\n",
    "    plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "    \n",
    "    # Loop through subplots to plot each neuron\n",
    "    ax = plt.gca()\n",
    "    data_means = torch.mean(ao_data[:t3, neuron_i, :, :], dim=2)\n",
    "    data_sems = torch.std(ao_data[:t3, neuron_i, :, :], dim=2) / ((data_means.shape[1]-1)**0.5)\n",
    "    conditions = [0, 7, 15, 23]\n",
    "    for j, condition in enumerate(conditions):\n",
    "        color = colors[j]\n",
    "        mean = data_means[:, condition]\n",
    "        sem = data_sems[:, condition]*1.96\n",
    "        ax.fill_between(range(len(data_means)), mean-sem, mean+sem, color=color, alpha=0.3, linewidth=0)\n",
    "        ax.plot(range(len(data_means)), mean, \"-\", linewidth=3, color=color, label=f\"{j}\")\n",
    "    \n",
    "    #ax.set_title(f\"neuron #{neuron_index}\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.axvspan(t1, t2, facecolor=\"k\", alpha=0.1)\n",
    "    ax.set_ylabel(\"activity\")\n",
    "    ax.set_xlabel(\"time (ms)\")\n",
    "    ax.set_xticks([t1, t2, t3])\n",
    "    ax.set_xticklabels([0, 100, t3*10])\n",
    "    ax.set_xlim(10, t3+5)\n",
    "    #ax.set_xlim(0, 2.16)\n",
    "    #ax.set_xlim(0, 2.3)\n",
    "    #ax.set_ylim(0, 55)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_dir_prefix+directory[5:-1] + f\"_exampleneuron.pdf\", bbox_inches='tight')\n",
    "def generate_exampleneurontuningcurve_figure(neuron_i=19):\n",
    "    cue_from, cue_to = t1, t2\n",
    "    delay_from, delay_to = t2+20, t3\n",
    "    \n",
    "    n_conditions = ORI_SET_SIZE\n",
    "    cue_data_means = np.zeros((n_conditions, ))\n",
    "    cue_data_sems = np.zeros((n_conditions, ))\n",
    "    delay_data_means = np.zeros((n_conditions, ))\n",
    "    delay_data_sems = np.zeros((n_conditions, ))\n",
    "    for i in np.arange(n_conditions):\n",
    "        cue_data = torch.mean(ao_data[cue_from:cue_to, neuron_i, i, :], dim=0)\n",
    "        cue_data_means[i] = torch.mean(cue_data)\n",
    "        cue_data_sems[i] = scipy.stats.sem(cue_data)\n",
    "        delay_data = torch.mean(ao_data[delay_from:delay_to, neuron_i, i, :], dim=0)\n",
    "        delay_data_means[i] = torch.mean(delay_data)\n",
    "        delay_data_sems[i] = scipy.stats.sem(delay_data)\n",
    "    thetas = np.arange(n_conditions)/n_conditions*360\n",
    "    \n",
    "    conditions = np.arange(0, n_conditions, 1)\n",
    "    thetas = thetas[conditions]\n",
    "    cue_data_means = cue_data_means[conditions]\n",
    "    cue_data_sems = cue_data_sems[conditions]\n",
    "    delay_data_means = delay_data_means[conditions]\n",
    "    delay_data_sems = delay_data_sems[conditions]\n",
    "    \n",
    "    thetas = np.hstack((thetas-360, thetas, thetas+360))\n",
    "    cue_data_means = np.hstack((cue_data_means, cue_data_means, cue_data_means))\n",
    "    cue_data_sems = np.hstack((cue_data_sems, cue_data_sems, cue_data_sems))\n",
    "    delay_data_means = np.hstack((delay_data_means, delay_data_means, delay_data_means))\n",
    "    delay_data_sems = np.hstack((delay_data_sems, delay_data_sems, delay_data_sems))\n",
    "    \n",
    "    # cue_data_sems /= np.max(cue_data_means)\n",
    "    # cue_data_means /= np.max(cue_data_means)\n",
    "    # delay_data_sems /= np.max(delay_data_means)\n",
    "    # delay_data_means /= np.max(delay_data_means)\n",
    "    \n",
    "    fig = plt.figure(figsize=(4, 2.5))\n",
    "    plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "    ax = plt.gca()\n",
    "    color=targetinput_color\n",
    "    ax.fill_between(thetas, cue_data_means-cue_data_sems, cue_data_means+cue_data_sems, color=color, alpha=0.3, linewidth=0, label=\"t=cue\")\n",
    "    ax.plot(thetas, cue_data_means, \"-\", linewidth=3, color=color)\n",
    "    color=distractor_color\n",
    "    ax.fill_between(thetas, delay_data_means-delay_data_sems, delay_data_means+delay_data_sems, color=color, alpha=0.3, linewidth=0, label=\"t=delay\")\n",
    "    ax.plot(thetas, delay_data_means, \"-\", linewidth=3, color=color)\n",
    "    \n",
    "    plt.xticks(np.array([-180, 0, 180, 360]))\n",
    "    plt.xlim(-180, 180)\n",
    "    plt.xlabel(\"stimulus angle\")\n",
    "    plt.gca().xaxis.set_major_formatter(FormatStrFormatter('%d째'))\n",
    "    plt.ylim(-.1, .99)\n",
    "    #ax.set_ylim(0, 55)\n",
    "    plt.ylabel(\"activity\")\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_dir_prefix+directory[5:-1] + f\"_exampletuningcurve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807e0127-f65d-4842-8b18-e9b0757475a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\n",
      "mean squared error (deg):  5.130105243815381\n",
      "Parameter containing:\n",
      "tensor([ 0.0766,  0.4909, -0.3033], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for _directory in directories:\n",
    "    print(_directory)\n",
    "    directory = _directory\n",
    "    \n",
    "    # Define the file path and module name\n",
    "    file_path = directory + \"task_and_training.py\"  # Change this to your file's path\n",
    "    module_name = 'task_and_training'  # Arbitrary name for the module\n",
    "    # Create a module object\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    # Load the module into memory and execute it\n",
    "    spec.loader.exec_module(module)\n",
    "    # Inject attributes into the global namespace\n",
    "    for attr in dir(module):\n",
    "        globals()[attr] = getattr(module, attr)\n",
    "    directory = _directory  # it might have been changed by the imported file, so change it back\n",
    "    \n",
    "    with open(f\"{directory}info.json\", 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "    hyperparameters = j[\"hyperparameters\"]\n",
    "    task_parameters = j[\"task_parameters\"]\n",
    "    model_parameters = j[\"model_parameters\"]\n",
    "    additional_comments = j[\"additional_comments\"]\n",
    "    directory = j[\"directory\"]\n",
    "        \n",
    "    # some noise may be necessary to prevent dividing by 0 in some of the analyses\n",
    "    # if noise_amplitude == 0: noise_amplitude = 0.0001\n",
    "    \n",
    "    task = Task()\n",
    "    model = Model()\n",
    "    if analyze_network.lower() == \"best\": network_filename = \"model_best.pth\"\n",
    "    elif analyze_network.lower() == \"final\": network_filename = f\"model_parameterupdate{hyperparameters['train_for_steps']}.pth\"\n",
    "    else: network_filename = f\"model_parameterupdate{analyze_network}.pth\"\n",
    "    model_state_dict = torch.load(directory+network_filename, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    resolution = 30\n",
    "    ORI_RES = 360//resolution\n",
    "    ORI_SET = torch.arange(0, 360, ORI_RES)\n",
    "    ORI_SET_SIZE = ORI_SET.shape[0]\n",
    "    \n",
    "    # fix delays at median values for analysis\n",
    "    delay0, delay1, delay2 = task.get_median_delays()\n",
    "    #delay1 = task_parameters[\"delay1_to\"]  # max delay1 (to ensure convergence to final state for analysis)\n",
    "    show_direction_for = task_parameters[\"show_direction_for\"]\n",
    "    show_cue_for = task_parameters[\"show_cue_for\"]\n",
    "    total_time = show_direction_for+show_cue_for+delay0+delay2\n",
    "    t1, t1d = delay0, \"before O1 presented\"\n",
    "    t1_5, t1_5d = delay0+show_direction_for//2, \"amid 01 presentation\"\n",
    "    t2, t2d = delay0+show_direction_for, \"after O1 presented\"\n",
    "    t3, t3d = delay0+show_direction_for+delay1, \"before O2 presented\"\n",
    "    t3_5, t3_5d = delay0+show_direction_for+delay1+show_direction_for//2, \"amid O2 presentation\"\n",
    "    t4, t4d = delay0+show_direction_for+delay1+show_direction_for, \"after O2 presented\"\n",
    "    t5, t5d = delay0+show_direction_for+delay2, \"before go cue\"\n",
    "    t6, t6d = total_time-1, \"at end of task\"\n",
    "    \n",
    "    # run the model on all possible directions \n",
    "    ao_input, ao_target, ao_mask = task.make_all_integer_directions_batch(delay0, delay1, delay2, distractor_probability=distractor_probability, resolution=resolution)\n",
    "    ao_noise_mask = task.get_noise_mask(delay0, delay1, delay2)\n",
    "    ao_noise_mask = ao_noise_mask.repeat(ao_input.shape[0], 1).unsqueeze(2).repeat(1, 1, model.dim_recurrent)  # convert to (batch_size, total_time, dim_recurrent)\n",
    "    ao_noise = torch.randn_like(ao_noise_mask) * ao_noise_mask * noise_amplitude\n",
    "    ao_output, ao_h = model.forward(ao_input, noise=ao_noise)\n",
    "    \n",
    "    # output model errors (with noise and without)\n",
    "    mse_o1, mse_o2, err_o1, err_o2 = task.calculate_errors(ao_target, ao_output, ao_mask, t5, t6)\n",
    "    ao_output_nn, ao_h_nn = model.forward(ao_input, noise=ao_noise*0)\n",
    "    mse_o1_nn, mse_o2_nn, err_o1_nn, err_o2_nn = task.calculate_errors(ao_target, ao_output_nn, ao_mask, t5, t6)\n",
    "    print(\"mean squared error (deg): \", err_o1)\n",
    "    if err_o1>15: \n",
    "        print(\"---NETWORK DISCARDED---\")\n",
    "        continue\n",
    "        \n",
    "    # for every timestep and every unit, calculate its activity in all trials\n",
    "    ao_data = torch.zeros((total_time, model.dim_recurrent, ORI_SET_SIZE, ORI_SET_SIZE))\n",
    "    for direction1 in range(ORI_SET_SIZE):\n",
    "        for direction2 in range(ORI_SET_SIZE):\n",
    "            o = ao_h[direction1 * ORI_SET_SIZE + direction2]\n",
    "            ao_data[:, :, direction1, direction2] = o\n",
    "            \n",
    "    # detach from autograd\n",
    "    ao_output = ao_output.detach()\n",
    "    ao_h = ao_h.detach()\n",
    "    ao_data = ao_data.detach()\n",
    "    \n",
    "    timestep, timestep_description = t5, t5d\n",
    "    cutoff_criterion = \"box\" # options: ratio, box\n",
    "    ring_cutoff = 2  # if ratio: minumum variance ratio to consider unit a ring unit\n",
    "    min_pri_var = 0.15  # if box: minimum variance in primary direction to consider unit a ring unit\n",
    "    max_sec_var = 0.10  # if box: maximum variance in the other direction to consider unit a ring unit\n",
    "    var_1 = torch.var(torch.mean(ao_data[timestep], dim=2), dim=1)**0.5 + 0.01\n",
    "    var_2 = torch.var(torch.mean(ao_data[timestep], dim=1), dim=1)**0.5 + 0.01\n",
    "    if cutoff_criterion == \"ratio\":\n",
    "        R1_i = torch.where(var_1/var_2 > ring_cutoff)[0]\n",
    "        R2_i = torch.where(var_2/var_1 > ring_cutoff)[0]\n",
    "        DT_i = torch.tensor([x for x in range(model.dim_recurrent) if (x not in R1_i) and (x not in R2_i)], dtype=int)\n",
    "    elif cutoff_criterion == \"box\":\n",
    "        R1_i = torch.where(torch.logical_and(var_1>min_pri_var, var_2<max_sec_var))[0]\n",
    "        R2_i = torch.where(torch.logical_and(var_2>min_pri_var, var_1<max_sec_var))[0]\n",
    "        DT_i = torch.tensor([x for x in range(model.dim_recurrent) if (x not in R1_i) and (x not in R2_i)], dtype=int)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    def calc_pref(units_i, timestep=t5, to=1, data=None, round_prefs=False):\n",
    "        if data is None: data = ao_data\n",
    "        w = torch.sum(data[timestep][units_i], dim=3-to).detach().numpy()\n",
    "        a = np.angle(np.sum(w*np.exp(1j*(np.arange(ORI_SET_SIZE)/ORI_SET_SIZE*2*np.pi)).reshape(1, -1), axis=1)/ (np.sum(np.abs(w), axis=1)+0.01)) * 180 / np.pi\n",
    "        a[a<0] = a[a<0]+360\n",
    "        a = torch.tensor(a)\n",
    "        if round_prefs: a = torch.round(a)\n",
    "        return a\n",
    "    prefs_1 = []  # every unit's preferred O1\n",
    "    prefs_2 = []  # every unit's preferred O2\n",
    "    for timestep in range(total_time):\n",
    "        prefs_1.append(calc_pref(range(model.dim_recurrent), timestep=timestep, to=1).unsqueeze(0))\n",
    "        prefs_2.append(calc_pref(range(model.dim_recurrent), timestep=timestep, to=2).unsqueeze(0))\n",
    "    prefs_1 = torch.cat(prefs_1)\n",
    "    prefs_2 = torch.cat(prefs_2)\n",
    "    \n",
    "    # sort units according to their preferred directions (don't sort DT)\n",
    "    R1_pref = prefs_1[t5-1][R1_i]\n",
    "    R2_pref = prefs_2[t5-1][R2_i]\n",
    "    DT_pref = prefs_1[t2-1][DT_i]\n",
    "    R1_i = R1_i.clone()[torch.argsort(R1_pref)]\n",
    "    R1_pref = R1_pref.clone()[torch.argsort(R1_pref)]\n",
    "    R2_i = R2_i.clone()[torch.argsort(R2_pref)]\n",
    "    R2_pref = R2_pref.clone()[torch.argsort(R2_pref)]\n",
    "    DT_i = DT_i.clone()[torch.argsort(DT_pref)]\n",
    "    DT_pref = DT_pref.clone()[torch.argsort(DT_pref)]\n",
    "    order_indices = torch.cat((R1_i, DT_i, R2_i))\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    break\n",
    "    generate_functional_connectivity_plot()\n",
    "    plt.clf()\n",
    "    plt.close()\n",
    "    continue\n",
    "    \n",
    "    if 'backprop' in directory:  # change vmax and pick a different trial to show the effect more clearly\n",
    "        generate_activity_figure(vmax=.7, trial_input=20, trial_distractor=16)\n",
    "    else:\n",
    "        generate_activity_figure()\n",
    "    generate_prefchange_figure()\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d964e07f-d298-429e-8b66-be3013c4d9bd",
   "metadata": {},
   "source": [
    "## just collect r values for ratio network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48239451-ee3d-4152-afc9-5665a63dd4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\n",
      "mean squared error (deg):  8.40837212192245\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r1/\n",
      "mean squared error (deg):  7.136185282127936\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r2/\n",
      "mean squared error (deg):  5.0318095210037574\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r3/\n",
      "mean squared error (deg):  6.963227003620884\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r4/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    f\"data/hdratioCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{i}/\" for i in range(30)\n",
    "]\n",
    "rs = []\n",
    "for _directory in directories:\n",
    "    print(_directory)\n",
    "    directory = _directory\n",
    "    \n",
    "    # Define the file path and module name\n",
    "    file_path = directory + \"task_and_training.py\"  # Change this to your file's path\n",
    "    module_name = 'task_and_training'  # Arbitrary name for the module\n",
    "    # Create a module object\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    # Load the module into memory and execute it\n",
    "    spec.loader.exec_module(module)\n",
    "    # Inject attributes into the global namespace\n",
    "    for attr in dir(module):\n",
    "        globals()[attr] = getattr(module, attr)\n",
    "    directory = _directory  # it might have been changed by the imported file, so change it back\n",
    "    \n",
    "    with open(f\"{directory}info.json\", 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "    hyperparameters = j[\"hyperparameters\"]\n",
    "    task_parameters = j[\"task_parameters\"]\n",
    "    model_parameters = j[\"model_parameters\"]\n",
    "    additional_comments = j[\"additional_comments\"]\n",
    "    directory = j[\"directory\"]\n",
    "        \n",
    "    # some noise may be necessary to prevent dividing by 0 in some of the analyses\n",
    "    # if noise_amplitude == 0: noise_amplitude = 0.0001\n",
    "    \n",
    "    task = Task()\n",
    "    model = Model()\n",
    "    if analyze_network.lower() == \"best\": network_filename = \"model_best.pth\"\n",
    "    elif analyze_network.lower() == \"final\": network_filename = f\"model_parameterupdate{hyperparameters['train_for_steps']}.pth\"\n",
    "    else: network_filename = f\"model_parameterupdate{analyze_network}.pth\"\n",
    "    model_state_dict = torch.load(directory+network_filename, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    top_parameters = (model.top_parameters)\n",
    "    r1 = top_parameters[0] / top_parameters[1]\n",
    "    r2 = top_parameters[0] / top_parameters[1]\n",
    "    rs.append(r.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1767d1c8-059f-4bbb-b9b7-c30db6e4f983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15394371052583058, 0.003004567577801103)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = np.array(rs)\n",
    "np.mean(rs), np.std(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e23aba-9f0e-46ff-ab92-3f54939763f7",
   "metadata": {},
   "source": [
    "## just collect ratios functional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "385de505-b0dd-4482-a3b0-60beeb485142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r0/\n",
      "mean squared error (deg):  8.40837212192245\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r1/\n",
      "mean squared error (deg):  7.136185282127936\n",
      "data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r2/\n",
      "mean squared error (deg):  5.0318095210037574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directories = [\n",
    "    f\"data/hdreshuffle_fCTRNN_2DIR1O_dr100_n0.1_la0_sa130_e1_dp1.0_r{i}/\" for i in range(3)\n",
    "]\n",
    "\n",
    "def generate_functional_connectivity_plot__justcollectfactor(dir_prefix=None, sd=False):\n",
    "    \"\"\"\n",
    "        Generates the plot\n",
    "        if sd is True, the shaded region represents SD of the distribution within each bin;\n",
    "        otherwise, it's 95%CI\n",
    "    \"\"\"\n",
    "    if dir_prefix is None:\n",
    "        dir_prefix=figure_dir_prefix\n",
    "    lim = 0.1\n",
    "    # function that will be fit to the averaged connection weights\n",
    "    def cosine_fit(x, a, b):\n",
    "        return a * np.cos(x * np.pi / 180) + b\n",
    "    \n",
    "    def get_connplot_graph(units1_id=None, unit1_pref=None, units2_id=None, unit2_pref=None, sm=0):\n",
    "        weight_matrix = None  # different models may store weights differently\n",
    "        try:\n",
    "            weight_matrix = model.fc_h2ah.weight\n",
    "        except:\n",
    "            weight_matrix = model.W_h_ah\n",
    "        distances_weights = {}\n",
    "        distances = []\n",
    "        weights = []\n",
    "        for i in range(len(units1_id)):\n",
    "            for j in range(len(units2_id)):\n",
    "                for k in range(-sm // 2, sm // 2 + 1):\n",
    "                    if j == i: continue\n",
    "                    diff = (unit2_pref[j] - unit1_pref[i]).item()\n",
    "                    if diff > 180: diff -= 360\n",
    "                    if diff < -180: diff += 360\n",
    "                    diff += k\n",
    "                    w_ij = weight_matrix[units2_id[j], units1_id[i]]\n",
    "                    distances.append(diff)\n",
    "                    weights.append(w_ij.item())\n",
    "        return np.array(distances), np.array(weights)\n",
    "    \n",
    "    def get_connplot_iu_graph(units_id, unit_pref, sm=0):\n",
    "        weight_matrix = None  # different models may store weights differently\n",
    "        try:\n",
    "            weight_matrix = model.fc_x2ah.weight\n",
    "        except:\n",
    "            weight_matrix = model.W_x_ah\n",
    "        distances_weights = {}\n",
    "        distances = []\n",
    "        weights = []\n",
    "        for i in range(len(units_id)):\n",
    "            for j in range(task_parameters[\"input_direction_units\"]):\n",
    "                for k in range(-sm // 2, sm // 2 + 1):\n",
    "                    # if j == i: continue\n",
    "                    diff = (unit_pref[i] - round(360 * j / task_parameters[\"input_direction_units\"])).item()\n",
    "                    if diff > 180: diff -= 360\n",
    "                    if diff < -180: diff += 360\n",
    "                    diff += k\n",
    "    \n",
    "                    w_ij = weight_matrix[units_id[i], j]\n",
    "                    distances.append(diff)\n",
    "                    weights.append(w_ij.item())\n",
    "        return np.array(distances), np.array(weights)\n",
    "    \n",
    "    def plot_weights(timestep_from, timestep_to, ax, weights='input', color=\"green\", lim=0.1, fit_curves=True, ax_i=-1):\n",
    "        if weights=='input':\n",
    "            x, y = get_connplot_iu_graph(R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1))\n",
    "        elif weights=='recurrent':\n",
    "            x, y = get_connplot_graph(R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1), R1_i, _calc_pref_ave(R1_i, timestep_from, timestep_to, to=1))\n",
    "        bins = np.linspace(-180, 180, 20)\n",
    "        x_binned = []\n",
    "        y_binned = []\n",
    "        sem_binned = []\n",
    "        bin_ids = np.digitize(x, bins)\n",
    "        for i in range(1, len(bins)):\n",
    "            x_binned.append(np.mean(x[bin_ids == i]))\n",
    "            y_binned.append(np.mean(y[bin_ids == i]))\n",
    "            sem_binned.append(scipy.stats.sem(y[bin_ids == i]) if not sd else np.std(y[bin_ids == i]))\n",
    "        x_binned, y_binned, sem_binned = np.array(x_binned), np.array(y_binned), np.array(sem_binned) * 1.96\n",
    "        ax.axhline(y=0.0, color='gray', linestyle='--', linewidth=2)\n",
    "        ax.fill_between(x_binned, y_binned - sem_binned, y_binned + sem_binned, color=color, alpha=0.3, linewidth=0)\n",
    "        ax.plot(x_binned, y_binned, \"-\", color=color, linewidth=3)\n",
    "        ax.set_xlim(-180, 180)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.set_ylim(-lim * 1.1, lim * 1.1)\n",
    "        ax.set_yticks([-lim, 0, lim])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([-180, 0, 180])\n",
    "        ax.xaxis.set_major_formatter(FormatStrFormatter('%d째'))\n",
    "        if ax_i>=1:\n",
    "            ax.set_xlabel(\"preferred angle difference\\nbetween two units\")\n",
    "        ax.set_yticks([-lim, 0, lim])\n",
    "\n",
    "        if not np.isfinite(x_binned).all():\n",
    "            print(\"x\")\n",
    "            print(x_binned)\n",
    "            print(weights)\n",
    "        if not np.isfinite(y_binned).all():\n",
    "            print(\"y\")\n",
    "            print(y_binned)\n",
    "        params, covariance = curve_fit(cosine_fit, x_binned, y_binned)\n",
    "        y_fit = cosine_fit(x_binned, *params)\n",
    "        if fit_curves:\n",
    "            ax.plot(x_binned, y_fit, \"--\", color=color, linewidth=3, label=f\"$a_{ax_i}$ = {params[0]:.3f}\")\n",
    "        return params\n",
    "\n",
    "    for fit_curves in [True, False]:\n",
    "        fig = plt.figure(figsize=(6, 6))\n",
    "        plt.rc('font', **{'family': 'Arial', 'weight': 'normal', 'size': 14})\n",
    "        ax1 = fig.add_subplot(2, 2, 1)\n",
    "        ax2 = fig.add_subplot(2, 2, 2)\n",
    "        ax3 = fig.add_subplot(2, 2, 3)\n",
    "        ax4 = fig.add_subplot(2, 2, 4)\n",
    "        ax1.set_ylabel(\"weight between two\\nrecurrent units\")\n",
    "        ax3.set_ylabel(\"weight between input\\nand recurrent unit\")\n",
    "        params_recurrent_cue = plot_weights(t1, t2, ax1, weights='recurrent', color=\"gray\", lim=lim, fit_curves=fit_curves)\n",
    "        params_recurrent_delay = plot_weights(t2+20, t3, ax2, weights='recurrent', color=\"k\", lim=lim, fit_curves=fit_curves, ax_i=0)\n",
    "        params_input_cue = plot_weights(t1, t2, ax3, weights='input', color=targetinput_color, lim=lim, fit_curves=fit_curves, ax_i=1)\n",
    "        params_input_delay = plot_weights(t2+20, t3, ax4, weights='input', color=distractor_color, lim=lim, fit_curves=fit_curves, ax_i=2)\n",
    "        if fit_curves: \n",
    "            for ax in [ax2, ax3, ax4]: ax.legend(loc='lower center')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(dir_prefix+directory[5:-1] + f\"_connectivity{'_nofit' if not fit_curves else ''}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.clf()\n",
    "    \n",
    "    factors = {\n",
    "        \"structural_factor\": params_recurrent_delay[0] / params_input_cue[0],\n",
    "        \"functional_factor\": params_input_cue[0] / params_input_delay[0]\n",
    "    }\n",
    "    return factors\n",
    "\n",
    "rs = []\n",
    "for _directory in directories:\n",
    "    print(_directory)\n",
    "    directory = _directory\n",
    "    \n",
    "    # Define the file path and module name\n",
    "    file_path = directory + \"task_and_training.py\"  # Change this to your file's path\n",
    "    module_name = 'task_and_training'  # Arbitrary name for the module\n",
    "    # Create a module object\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    # Load the module into memory and execute it\n",
    "    spec.loader.exec_module(module)\n",
    "    # Inject attributes into the global namespace\n",
    "    for attr in dir(module):\n",
    "        globals()[attr] = getattr(module, attr)\n",
    "    directory = _directory  # it might have been changed by the imported file, so change it back\n",
    "    \n",
    "    with open(f\"{directory}info.json\", 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "    hyperparameters = j[\"hyperparameters\"]\n",
    "    task_parameters = j[\"task_parameters\"]\n",
    "    model_parameters = j[\"model_parameters\"]\n",
    "    additional_comments = j[\"additional_comments\"]\n",
    "    directory = j[\"directory\"]\n",
    "        \n",
    "    # some noise may be necessary to prevent dividing by 0 in some of the analyses\n",
    "    # if noise_amplitude == 0: noise_amplitude = 0.0001\n",
    "    \n",
    "    task = Task()\n",
    "    model = Model()\n",
    "    if analyze_network.lower() == \"best\": network_filename = \"model_best.pth\"\n",
    "    elif analyze_network.lower() == \"final\": network_filename = f\"model_parameterupdate{hyperparameters['train_for_steps']}.pth\"\n",
    "    else: network_filename = f\"model_parameterupdate{analyze_network}.pth\"\n",
    "    model_state_dict = torch.load(directory+network_filename, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    resolution = 30\n",
    "    ORI_RES = 360//resolution\n",
    "    ORI_SET = torch.arange(0, 360, ORI_RES)\n",
    "    ORI_SET_SIZE = ORI_SET.shape[0]\n",
    "    \n",
    "    # fix delays at median values for analysis\n",
    "    delay0, delay1, delay2 = task.get_median_delays()\n",
    "    #delay1 = task_parameters[\"delay1_to\"]  # max delay1 (to ensure convergence to final state for analysis)\n",
    "    show_direction_for = task_parameters[\"show_direction_for\"]\n",
    "    show_cue_for = task_parameters[\"show_cue_for\"]\n",
    "    total_time = show_direction_for+show_cue_for+delay0+delay2\n",
    "    t1, t1d = delay0, \"before O1 presented\"\n",
    "    t1_5, t1_5d = delay0+show_direction_for//2, \"amid 01 presentation\"\n",
    "    t2, t2d = delay0+show_direction_for, \"after O1 presented\"\n",
    "    t3, t3d = delay0+show_direction_for+delay1, \"before O2 presented\"\n",
    "    t3_5, t3_5d = delay0+show_direction_for+delay1+show_direction_for//2, \"amid O2 presentation\"\n",
    "    t4, t4d = delay0+show_direction_for+delay1+show_direction_for, \"after O2 presented\"\n",
    "    t5, t5d = delay0+show_direction_for+delay2, \"before go cue\"\n",
    "    t6, t6d = total_time-1, \"at end of task\"\n",
    "    \n",
    "    # run the model on all possible directions \n",
    "    ao_input, ao_target, ao_mask = task.make_all_integer_directions_batch(delay0, delay1, delay2, distractor_probability=distractor_probability, resolution=resolution)\n",
    "    ao_noise_mask = task.get_noise_mask(delay0, delay1, delay2)\n",
    "    ao_noise_mask = ao_noise_mask.repeat(ao_input.shape[0], 1).unsqueeze(2).repeat(1, 1, model.dim_recurrent)  # convert to (batch_size, total_time, dim_recurrent)\n",
    "    ao_noise = torch.randn_like(ao_noise_mask) * ao_noise_mask * noise_amplitude\n",
    "    ao_output, ao_h = model.forward(ao_input, noise=ao_noise)\n",
    "    \n",
    "    # output model errors (with noise and without)\n",
    "    mse_o1, mse_o2, err_o1, err_o2 = task.calculate_errors(ao_target, ao_output, ao_mask, t5, t6)\n",
    "    ao_output_nn, ao_h_nn = model.forward(ao_input, noise=ao_noise*0)\n",
    "    mse_o1_nn, mse_o2_nn, err_o1_nn, err_o2_nn = task.calculate_errors(ao_target, ao_output_nn, ao_mask, t5, t6)\n",
    "    print(\"mean squared error (deg): \", err_o1)\n",
    "    if err_o1>15: \n",
    "        print(\"---NETWORK DISCARDED---\")\n",
    "        continue\n",
    "\n",
    "    factors = generate_functional_connectivity_plot__justcollectfactor()\n",
    "    rs.append(factors['structural_factor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cefe5a5-1a7f-4287-ab62-2de5184d72ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9996282186388511, 0.006837723074401044)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = np.array(rs)\n",
    "np.mean(rs), np.std(rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7485491f-54b9-41dd-aef9-ea65de38b1b7",
   "metadata": {},
   "source": [
    "# (Temporary) Add all structural and functional ratio info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c469cb18-fc3a-4855-b14d-dd560d096c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r0/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r1/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r2/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r3/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r4/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r5/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r6/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r7/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r8/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r9/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r10/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r11/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r12/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r13/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r14/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r15/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r16/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r17/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r18/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/47/5g669ycs2t17c0kkg2_np43m0000gn/T/ipykernel_73287/1567386492.py:308: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(figsize=(6, 6))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r19/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r20/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r21/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r22/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r23/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r24/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r25/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r26/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r27/\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r28/\n",
      "ERROR >15\n",
      "data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r29/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directories = [f\"data/backpropCTRNN_2DIR1O_dr100_n0.1_la0_e1_dp1.0_r{k}/\" for k in range(30)]\n",
    "for _directory in directories:\n",
    "    print(_directory)\n",
    "    directory = _directory\n",
    "    \n",
    "    # Define the file path and module name\n",
    "    file_path = directory + \"task_and_training.py\"  # Change this to your file's path\n",
    "    module_name = 'task_and_training'  # Arbitrary name for the module\n",
    "    # Create a module object\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    # Load the module into memory and execute it\n",
    "    spec.loader.exec_module(module)\n",
    "    # Inject attributes into the global namespace\n",
    "    for attr in dir(module):\n",
    "        globals()[attr] = getattr(module, attr)\n",
    "    directory = _directory  # it might have been changed by the imported file, so change it back\n",
    "    \n",
    "    with open(f\"{directory}info.json\", 'r', encoding='utf-8') as f:\n",
    "        j = json.load(f)\n",
    "    hyperparameters = j[\"hyperparameters\"]\n",
    "    task_parameters = j[\"task_parameters\"]\n",
    "    model_parameters = j[\"model_parameters\"]\n",
    "    additional_comments = j[\"additional_comments\"]\n",
    "    directory = j[\"directory\"]\n",
    "        \n",
    "    # some noise may be necessary to prevent dividing by 0 in some of the analyses\n",
    "    # if noise_amplitude == 0: noise_amplitude = 0.0001\n",
    "    \n",
    "    task = Task()\n",
    "    model = Model()\n",
    "    if analyze_network.lower() == \"best\": network_filename = \"model_best.pth\"\n",
    "    elif analyze_network.lower() == \"final\": network_filename = f\"model_parameterupdate{hyperparameters['train_for_steps']}.pth\"\n",
    "    else: network_filename = f\"model_parameterupdate{analyze_network}.pth\"\n",
    "    model_state_dict = torch.load(directory+network_filename, map_location=torch.device('cpu'))[\"model_state_dict\"]\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    \n",
    "    resolution = 30\n",
    "    ORI_RES = 360//resolution\n",
    "    ORI_SET = torch.arange(0, 360, ORI_RES)\n",
    "    ORI_SET_SIZE = ORI_SET.shape[0]\n",
    "    \n",
    "    # fix delays at median values for analysis\n",
    "    delay0, delay1, delay2 = task.get_median_delays()\n",
    "    #delay1 = task_parameters[\"delay1_to\"]  # max delay1 (to ensure convergence to final state for analysis)\n",
    "    show_direction_for = task_parameters[\"show_direction_for\"]\n",
    "    show_cue_for = task_parameters[\"show_cue_for\"]\n",
    "    total_time = show_direction_for+show_cue_for+delay0+delay2\n",
    "    t1, t1d = delay0, \"before O1 presented\"\n",
    "    t1_5, t1_5d = delay0+show_direction_for//2, \"amid 01 presentation\"\n",
    "    t2, t2d = delay0+show_direction_for, \"after O1 presented\"\n",
    "    t3, t3d = delay0+show_direction_for+delay1, \"before O2 presented\"\n",
    "    t3_5, t3_5d = delay0+show_direction_for+delay1+show_direction_for//2, \"amid O2 presentation\"\n",
    "    t4, t4d = delay0+show_direction_for+delay1+show_direction_for, \"after O2 presented\"\n",
    "    t5, t5d = delay0+show_direction_for+delay2, \"before go cue\"\n",
    "    t6, t6d = total_time-1, \"at end of task\"\n",
    "    \n",
    "    # run the model on all possible directions \n",
    "    ao_input, ao_target, ao_mask = task.make_all_integer_directions_batch(delay0, delay1, delay2, distractor_probability=distractor_probability, resolution=resolution)\n",
    "    ao_noise_mask = task.get_noise_mask(delay0, delay1, delay2)\n",
    "    ao_noise_mask = ao_noise_mask.repeat(ao_input.shape[0], 1).unsqueeze(2).repeat(1, 1, model.dim_recurrent)  # convert to (batch_size, total_time, dim_recurrent)\n",
    "    ao_noise = torch.randn_like(ao_noise_mask) * ao_noise_mask * noise_amplitude\n",
    "    ao_output, ao_h = model.forward(ao_input, noise=ao_noise)\n",
    "    \n",
    "    # output model errors (with noise and without)\n",
    "    mse_o1, mse_o2, err_o1, err_o2 = task.calculate_errors(ao_target, ao_output, ao_mask, t5, t6)\n",
    "    ao_output_nn, ao_h_nn = model.forward(ao_input, noise=ao_noise*0)\n",
    "    mse_o1_nn, mse_o2_nn, err_o1_nn, err_o2_nn = task.calculate_errors(ao_target, ao_output_nn, ao_mask, t5, t6)\n",
    "    \n",
    "    # for every timestep and every unit, calculate its activity in all trials\n",
    "    ao_data = torch.zeros((total_time, model.dim_recurrent, ORI_SET_SIZE, ORI_SET_SIZE))\n",
    "    for direction1 in range(ORI_SET_SIZE):\n",
    "        for direction2 in range(ORI_SET_SIZE):\n",
    "            o = ao_h[direction1 * ORI_SET_SIZE + direction2]\n",
    "            ao_data[:, :, direction1, direction2] = o\n",
    "            \n",
    "    # detach from autograd\n",
    "    ao_output = ao_output.detach()\n",
    "    ao_h = ao_h.detach()\n",
    "    ao_data = ao_data.detach()\n",
    "    \n",
    "    timestep, timestep_description = t5, t5d\n",
    "    cutoff_criterion = \"box\" # options: ratio, box\n",
    "    ring_cutoff = 2  # if ratio: minumum variance ratio to consider unit a ring unit\n",
    "    min_pri_var = 0.15  # if box: minimum variance in primary direction to consider unit a ring unit\n",
    "    max_sec_var = 0.10  # if box: maximum variance in the other direction to consider unit a ring unit\n",
    "    var_1 = torch.var(torch.mean(ao_data[timestep], dim=2), dim=1)**0.5 + 0.01\n",
    "    var_2 = torch.var(torch.mean(ao_data[timestep], dim=1), dim=1)**0.5 + 0.01\n",
    "    if cutoff_criterion == \"ratio\":\n",
    "        R1_i = torch.where(var_1/var_2 > ring_cutoff)[0]\n",
    "        R2_i = torch.where(var_2/var_1 > ring_cutoff)[0]\n",
    "        DT_i = torch.tensor([x for x in range(model.dim_recurrent) if (x not in R1_i) and (x not in R2_i)], dtype=int)\n",
    "    elif cutoff_criterion == \"box\":\n",
    "        R1_i = torch.where(torch.logical_and(var_1>min_pri_var, var_2<max_sec_var))[0]\n",
    "        R2_i = torch.where(torch.logical_and(var_2>min_pri_var, var_1<max_sec_var))[0]\n",
    "        DT_i = torch.tensor([x for x in range(model.dim_recurrent) if (x not in R1_i) and (x not in R2_i)], dtype=int)\n",
    "    \n",
    "    ##############################################################\n",
    "    \n",
    "    def calc_pref(units_i, timestep=t5, to=1, data=None, round_prefs=False):\n",
    "        if data is None: data = ao_data\n",
    "        w = torch.sum(data[timestep][units_i], dim=3-to).detach().numpy()\n",
    "        a = np.angle(np.sum(w*np.exp(1j*(np.arange(ORI_SET_SIZE)/ORI_SET_SIZE*2*np.pi)).reshape(1, -1), axis=1)/ (np.sum(np.abs(w), axis=1)+0.01)) * 180 / np.pi\n",
    "        a[a<0] = a[a<0]+360\n",
    "        a = torch.tensor(a)\n",
    "        if round_prefs: a = torch.round(a)\n",
    "        return a\n",
    "    prefs_1 = []  # every unit's preferred O1\n",
    "    prefs_2 = []  # every unit's preferred O2\n",
    "    for timestep in range(total_time):\n",
    "        prefs_1.append(calc_pref(range(model.dim_recurrent), timestep=timestep, to=1).unsqueeze(0))\n",
    "        prefs_2.append(calc_pref(range(model.dim_recurrent), timestep=timestep, to=2).unsqueeze(0))\n",
    "    prefs_1 = torch.cat(prefs_1)\n",
    "    prefs_2 = torch.cat(prefs_2)\n",
    "    \n",
    "    # sort units according to their preferred directions (don't sort DT)\n",
    "    R1_pref = prefs_1[t5-1][R1_i]\n",
    "    R2_pref = prefs_2[t5-1][R2_i]\n",
    "    DT_pref = prefs_1[t2-1][DT_i]\n",
    "    R1_i = R1_i.clone()[torch.argsort(R1_pref)]\n",
    "    R1_pref = R1_pref.clone()[torch.argsort(R1_pref)]\n",
    "    R2_i = R2_i.clone()[torch.argsort(R2_pref)]\n",
    "    R2_pref = R2_pref.clone()[torch.argsort(R2_pref)]\n",
    "    DT_i = DT_i.clone()[torch.argsort(DT_pref)]\n",
    "    DT_pref = DT_pref.clone()[torch.argsort(DT_pref)]\n",
    "    order_indices = torch.cat((R1_i, DT_i, R2_i))\n",
    "    \n",
    "    ##############################################################\n",
    "    if err_o1 > 15: \n",
    "        print(\"ERROR >15\")\n",
    "        continue\n",
    "    generate_functional_connectivity_plot(dir_prefix=\"data_json/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492ba74a-b307-4476-9a31-df7df8d9641e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
